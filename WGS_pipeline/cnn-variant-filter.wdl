# This workflow takes an input CRAM to call variants with HaplotypeCaller
# Then filters the calls with the CNNVariant neural net tool
# The site-level scores are added to the INFO field of the VCF.
# The architecture arguments, info_key and tensor type arguments MUST be in agreement
# (e.g. 2D models must have tensor_type of read_tensor and info_key CNN_2D, 1D models have tensor_type reference and info_key CNN_1D)
# The INFO field key will be "1D_CNN" or "2D_CNN" depending on the neural net architecture used for inference.
# The architecture arguments specify pre-trained networks.
# New networks can be trained by the GATK tools: CNNVariantWriteTensors and CNNVariantTrain
# The CRAM could be generated by the single-sample pipeline
# (https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl)
# Also accepts a BAM as the input file in which case a BAM index is required as well.

import "https://raw.githubusercontent.com/broadinstitute/ccle_processing/master/WGS_pipeline/cnn-variant-common-tasks.wdl" as CNNTasks

workflow Cram2FilteredVcf {
    File input_file                  # Aligned CRAM file or Aligned BAM files
    File? input_file_index           # Index for an aligned BAM file if that is the input, unneeded if input is a CRAM
    File reference_fasta 
    File reference_dict
    File reference_fasta_index
    File resource_fofn               # File of VCF file names of resources of known SNPs and INDELs, (e.g. mills, gnomAD)
    File resource_fofn_index         # File of VCF file indices of resources
    File? architecture_json          # Neural Net configuration for CNNScoreVariants
    File? architecture_hd5           # Pre-Trained weights and architecture for CNNScoreVariants
    Int? inference_batch_size        # Batch size for python in CNNScoreVariants
    Int? transfer_batch_size         # Batch size for java in CNNScoreVariants
    Int? intra_op_threads            # Tensorflow threading within nodes
    Int? inter_op_threads            # Tensorflow threading between nodes
    String output_prefix             # Identifying string for this run will be used to name all output files
    String? tensor_type              # What kind of tensors the Neural Net expects (e.g. reference, read_tensor)
    String info_key                  # The score key for the info field of the vcf (e.g. CNN_1D, CNN_2D)
    String snp_tranches              # Filtering threshold(s) for SNPs in terms of sensitivity to overlapping known variants in resources
    String indel_tranches            # Filtering threshold(s) for INDELs in terms of sensitivity to overlapping known variants in resources
    File? gatk_override              # GATK Jar file to over ride the one included in gatk_docker
    String gatk_docker
    File calling_intervals
    Int scatter_count                # Number of shards for parallelization of HaplotypeCaller and CNNScoreVariants
    String extra_args                # Extra arguments for HaplotypeCaller

    # Runtime parameters
    Int? mem_gb
    Int? preemptible_attempts
    Float? disk_space_gb
    Int? cpu

    Int? increase_disk_size
    Int additional_disk = select_first([increase_disk_size, 20])
    Float ref_size = size(reference_fasta, "GB") + size(reference_fasta_index, "GB") + size(reference_dict, "GB")

    # Clunky check to see if the input is a BAM or a CRAM
    if (basename(input_file) == basename(input_file, ".bam")){
        call CNNTasks.CramToBam as CramToBam {
            input:
              reference_fasta = reference_fasta,
              reference_dict = reference_dict,
              reference_fasta_index = reference_fasta_index,
              cram_file = input_file,
              output_prefix = output_prefix,
              disk_space_gb = round(4*size(input_file, "GB") + ref_size + additional_disk),
              preemptible_attempts = preemptible_attempts
        }
    }

    call SplitNCigarReads_GATK4 {
        input:
          input_bam = bam_input,
          input_bam_index = bai_input,
          base_name = sampleName + ".split",
          ref_fasta = refFasta,
          ref_fasta_index = refFastaIndex,
          ref_dict = refDict,
          interval_list = wgsCallingIntervalList,
          preemptible_count = preemptible_count,
          docker = gatk4_docker,
          gatk_path = gatk_path
    }

    call BaseRecalibrator {
      input:
        input_bam = select_first([SplitNCigarReads_GATK4.output_bam, SplitNCigarReads.output_bam]),
        input_bam_index = select_first([SplitNCigarReads_GATK4.output_bam_index, SplitNCigarReads.output_bam_index]),
        recal_output_file = sampleName + ".recal_data.csv",
          dbSNP_vcf = dbSnpVcf,
          dbSNP_vcf_index = dbSnpVcfIndex,
          known_indels_sites_VCFs = knownVcfs,
          known_indels_sites_indices = knownVcfsIndices,
          ref_dict = refDict,
          ref_fasta = refFasta,
          ref_fasta_index = refFastaIndex,
          preemptible_count = preemptible_count,
        docker = gatk4_docker,
        gatk_path = gatk_path
    }

    call ApplyBQSR {
      input:
        input_bam =  select_first([SplitNCigarReads_GATK4.output_bam, SplitNCigarReads.output_bam]),
        input_bam_index = select_first([SplitNCigarReads_GATK4.output_bam_index, SplitNCigarReads.output_bam_index]),
        base_name = sampleName + ".aligned.duplicates_marked.recalibrated",
        ref_fasta = refFasta,
        ref_fasta_index = refFastaIndex,
        ref_dict = refDict,
        recalibration_report = BaseRecalibrator.recalibration_report,
        preemptible_count = preemptible_count,
        docker = gatk4_docker,
        gatk_path = gatk_path
    }

    call CNNTasks.SplitIntervals as SplitIntervals {
        input:
            gatk_override = gatk_override,
            scatter_count = scatter_count,
            intervals = calling_intervals,
            ref_fasta = reference_fasta,
            ref_dict = reference_dict,
            ref_fai = reference_fasta_index,
            gatk_docker = gatk_docker,
            disk_space = round(additional_disk + ref_size)
    }

    String input_bam = select_first([CramToBam.output_bam, input_file])
    Float bam_size = size(input_bam, "GB")

    scatter (calling_interval in SplitIntervals.interval_files) {
        call CNNTasks.RunHC4 as RunHC4 {
            input:
                input_bam = input_bam,
                input_bam_index = select_first([CramToBam.output_bam_index, input_file_index]),
                reference_fasta = reference_fasta,
                reference_dict = reference_dict,
                reference_fasta_index = reference_fasta_index,
                output_prefix = output_prefix,
                interval_list = calling_interval,
                gatk_docker = gatk_docker,
                gatk_override = gatk_override,
                preemptible_attempts = preemptible_attempts,
                extra_args = extra_args,
                disk_space_gb = round(bam_size + ref_size + additional_disk)
        }

        call CNNTasks.CNNScoreVariants as CNNScoreVariants {
            input:
                input_vcf = RunHC4.raw_vcf,
                input_vcf_index = RunHC4.raw_vcf_index,
                bam_file = RunHC4.bamout,
                bam_file_index = RunHC4.bamout_index,
                architecture_json = architecture_json,
                architecture_hd5 = architecture_hd5,
                reference_fasta = reference_fasta,
                tensor_type = tensor_type,
                inference_batch_size = inference_batch_size,
                transfer_batch_size = transfer_batch_size,
                intra_op_threads = intra_op_threads,
                inter_op_threads = inter_op_threads,
                reference_dict = reference_dict,
                reference_fasta_index = reference_fasta_index,               
                output_prefix = output_prefix,
                interval_list = calling_interval,
                gatk_override = gatk_override,
                gatk_docker = gatk_docker,
                preemptible_attempts = preemptible_attempts,
                mem_gb = mem_gb,
                disk_space_gb = round((bam_size/scatter_count) + ref_size + additional_disk)
        }
    }

    call CNNTasks.MergeVCFs as MergeVCF_HC4 {
        input: 
            input_vcfs = CNNScoreVariants.cnn_annotated_vcf,
            output_prefix = output_prefix,
            gatk_override = gatk_override,
            preemptible_attempts = preemptible_attempts,
            gatk_docker = gatk_docker,
            disk_space_gb = additional_disk
    }

    call CNNTasks.FilterVariantTranches as FilterVariantTranches {
        input:
            input_vcf = MergeVCF_HC4.merged_vcf,
            input_vcf_index = MergeVCF_HC4.merged_vcf_index,
            resource_fofn = resource_fofn,
            resource_fofn_index = resource_fofn_index,
            output_prefix = output_prefix,
            snp_tranches = snp_tranches,
            indel_tranches = indel_tranches,
            info_key = info_key,
            gatk_override = gatk_override,
            preemptible_attempts = preemptible_attempts,
            gatk_docker = gatk_docker,
            disk_space_gb = additional_disk
    }

    call CNNTasks.SamtoolsMergeBAMs as SamtoolsMergeBAMs {
        input:
            input_bams = RunHC4.bamout,
            output_prefix = output_prefix,
            disk_space_gb = round(bam_size + ref_size + additional_disk)
    }

    output {
        File cnn_filtered_vcf=FilterVariantTranches.cnn_filtered_vcf
        File cnn_filtered_vcf_index=FilterVariantTranches.cnn_filtered_vcf_index
    }
}

################################################################################

task Funcotate {

    # ==============
    # Inputs
    File ref_fasta
    File ref_fasta_index
    File ref_dict

    File input_vcf
    File input_vcf_idx

    String reference_version

    String output_file_base_name
    String output_format

    Boolean compress
    Boolean use_gnomad

    # This should be updated when a new version of the data sources is released
    # TODO: Make this dynamically chosen in the command.
    File? data_sources_tar_gz = "gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz"

    String? control_id
    String? case_id
    String? sequencing_center
    String? sequence_source
    String? transcript_selection_mode
    File? transcript_selection_list
    Array[String]? annotation_defaults
    Array[String]? annotation_overrides
    Array[String]? funcotator_excluded_fields
    Boolean? filter_funcotations
    File? interval_list

    String? extra_args

    # ==============
    # Process input args:

    String output_maf = output_file_base_name + ".maf"
    String output_maf_index = output_maf + ".idx"

    String output_vcf = output_file_base_name + if compress then ".vcf.gz" else ".vcf"
    String output_vcf_idx = output_vcf +  if compress then ".tbi" else ".idx"

    String output_file = if output_format == "MAF" then output_maf else output_vcf
    String output_file_index = if output_format == "MAF" then output_maf_index else output_vcf_idx

    String transcript_selection_arg = if defined(transcript_selection_list) then " --transcript-list " else ""
    String annotation_def_arg = if defined(annotation_defaults) then " --annotation-default " else ""
    String annotation_over_arg = if defined(annotation_overrides) then " --annotation-override " else ""
    String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then " --remove-filtered-variants " else ""
    String excluded_fields_args = if defined(funcotator_excluded_fields) then " --exclude-field " else ""

    String interval_list_arg = if defined(interval_list) then " -L " else ""

    String extra_args_arg = select_first([extra_args, ""])

    # ==============
    # Runtime options:
    String gatk_docker

    File? gatk_override
    Int? mem
    Int? preemptible_attempts
    Int? max_retries
    Int? disk_space_gb
    Int? cpu

    Boolean use_ssd = false

    # Mem is in units of GB but our command and memory runtime values are in MB
    Int default_ram_mb = 1024 * 3
    Int machine_mem = if defined(mem) then mem *1024 else default_ram_mb
    Int command_mem = machine_mem - 1024

    # Calculate disk size:
    Float ref_size_gb = size(ref_fasta, "GiB") + size(ref_fasta_index, "GiB") + size(ref_dict, "GiB")
    Float vcf_size_gb = size(input_vcf, "GiB") + size(input_vcf_idx, "GiB")
    Float ds_size_gb = size(data_sources_tar_gz, "GiB")

    Int default_disk_space_gb = ceil( ref_size_gb + (ds_size_gb * 2) + (vcf_size_gb * 10) ) + 20

    # Silly hack to allow us to use the dollar sign in the command section:
    String dollar = "$"

    command <<<
        set -e
        export GATK_LOCAL_JAR=${default="/root/gatk.jar" gatk_override}

        # =======================================
        # Hack to validate our WDL inputs:
        #
        # NOTE: This happens here so that we don't waste time copying down the data sources if there's an error.

        if [[ "${output_format}" != "MAF" ]] && [[ "${output_format}" != "VCF" ]] ; then
            echo "ERROR: Output format must be MAF or VCF."
        fi

        # =======================================
        # Handle our data sources:

        # Extract the tar.gz:
        echo "Extracting data sources tar/gzip file..."
        mkdir datasources_dir
        tar zxvf ${data_sources_tar_gz} -C datasources_dir --strip-components 1
        DATA_SOURCES_FOLDER="$PWD/datasources_dir"

        # Handle gnomAD:
        if ${use_gnomad} ; then
            echo "Enabling gnomAD..."
            for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do
                if [[ -f ${dollar}{DATA_SOURCES_FOLDER}/${dollar}{potential_gnomad_gz} ]] ; then
                    cd ${dollar}{DATA_SOURCES_FOLDER}
                    tar -zvxf ${dollar}{potential_gnomad_gz}
                    cd -
                else
                    echo "ERROR: Cannot find gnomAD folder: ${dollar}{potential_gnomad_gz}" 1>&2
                    false
                fi
            done
        fi

        # =======================================
        # Run Funcotator:
        gatk --java-options "-Xmx${command_mem}m" Funcotator \
            --data-sources-path $DATA_SOURCES_FOLDER \
            --ref-version ${reference_version} \
            --output-file-format ${output_format} \
            -R ${ref_fasta} \
            -V ${input_vcf} \
            -O ${output_file} \
            ${interval_list_arg} ${default="" interval_list} \
            --annotation-default normal_barcode:${default="Unknown" control_id} \
            --annotation-default tumor_barcode:${default="Unknown" case_id} \
            --annotation-default Center:${default="Unknown" sequencing_center} \
            --annotation-default source:${default="Unknown" sequence_source} \
            ${"--transcript-selection-mode " + transcript_selection_mode} \
            ${transcript_selection_arg}${default="" sep=" --transcript-list " transcript_selection_list} \
            ${annotation_def_arg}${default="" sep=" --annotation-default " annotation_defaults} \
            ${annotation_over_arg}${default="" sep=" --annotation-override " annotation_overrides} \
            ${excluded_fields_args}${default="" sep=" --exclude-field " funcotator_excluded_fields} \
            ${filter_funcotations_args} \
            ${extra_args_arg}

        # =======================================
        # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:
        if [[ "${output_format}" == "MAF" ]] ; then
            touch ${output_maf_index}
        fi
    >>>

    runtime {
        docker: gatk_docker
        bootDiskSizeGb: 20
        memory: machine_mem + " MB"
        disks: "local-disk " + select_first([disk_space_gb, default_disk_space_gb]) + if use_ssd then " SSD" else " HDD"
        preemptible: select_first([preemptible_attempts, 3])
        maxRetries: select_first([max_retries, 0])
        cpu: select_first([cpu, 1])
    }

    output {
        File funcotated_output_file = "${output_file}"
        File funcotated_output_file_index = "${output_file_index}"
    }
}

task run_vep {
  input {
    String ref # GRCh37 or GRCh38
    File cache_dir # path to location of cache files
    String cache_version = "100"
    File vcf
    File vcf_idx
    Int disk_size = 125 # test 100G for VEP?
  }


  String cache_dirname = basename(cache_dir, '.tar.gz')
  #String outprefix = basename(vcf, '.vcf')  
  String outprefix = basename(vcf, '.vcf.gz')  
  String outfname = "VEP_raw.~{outprefix}.vcf"

  command {

    echo "## extracting and localizing cache directory"
    time tar -xf ~{cache_dir} -C ~/
    CACHE_PATH=$(cd ~/~{cache_dirname}; pwd)
    echo "## success; see cache directory -- "$CACHE_PATH

    #plugins datasets to download, from: https://m.ensembl.org/info/docs/tools/vep/script/vep_options.html#basic
    curl ftp://dbnsfp:dbnsfp@dbnsfp.softgenetics.com/dbNSFP4.1a.zip --output dbNSFP4.1a.zip
    unzip dbNSFP4.1a.zip
    zcat dbNSFP4.1a_variant.chr1.gz | head -n1 > h
    zgrep -h -v ^#chr dbNSFP4.1a_variant.chr* | sort -T /path/to/tmp_folder -k1,1 -k2,2n - | cat h - | bgzip -c > dbNSFP4.1a_grch38.gz
    tabix -s 1 -b 2 -e 2 dbNSFP4.1a_grch38.gz

    curl https://raw.githubusercontent.com/Ensembl/VEP_plugins/release/104/ExACpLI_values.txt --output values_file.txt

    curl gs://ccle_default_params/mastermind_cited_variants_reference-2021.04.02-grch38-vcf.zip
    unzip mastermind_cited_variants_reference-XXXX.XX.XX-grch38-vcf.zip
    bgzip mastermind_cited_variants_reference-XXXX.XX.XX-GRCh38-vcf
    tabix -p vcf mastermind_cited_variants_reference-XXXX.XX.XX.GRCh38-vcf.gz
    
    curl https://raw.githubusercontent.com/Illumina/SpliceAI/master/spliceai/annotations/grch38.txt --output grch38spliceAI.txt
    
    /opt/vep/src/ensembl-vep/vep \
    --cache \
    --dir_cache "$CACHE_PATH" \
    --cache_version ~{cache_version} \
    --offline \
    --fork 4 \
    --format vcf \
    --vcf \
    --assembly ~{ref} \
    --input_file ~{vcf} \
    --output_file ~{outfname} \
    --no_stats \
    --pick \
    --everything \
    --plugin ExACpLI,values_file.txt \
    --plugin dbNSFP,dbNSFP4.1a.zip,LRT_score,GERP++_RS \
    --plugin FunMotifs \
    --plugin NearestGene \
    --plugin Mastermind,/path/to/mastermind_cited_variants_reference-XXXX.XX.XX.GRChXX-vcf.gz \
    --plugin SpliceAI,snv=/path/to/spliceai_scores.raw.snv.hg38.vcf.gz,indel=/path/to/spliceai_scores.raw.indel.hg38.vcf.gz

    rm -rf ~{cache_dir}
    rm -rf $CACHE_PATH
  }

  runtime {
    docker: "ensemblorg/ensembl-vep:latest"
    disks: "local-disk " + disk_size + " HDD"
    bootDiskSizeGb: disk_size
    memory: "16G"
    preemptible: 3
    maxRetries: 3
  }

  output {
    File vep_out = "~{outfname}"
  }

}
